name: gn1p

model:
  model:
    class_path: salt.models.JetTagger
    init_args:

      init_net:
        class_path: salt.models.Dense
        init_args:
          input_size: 23
          output_size: &embd_dim 128
          hidden_layers: [256]
          activation: &activation SiLU
          norm_layer: &norm_layer LayerNorm

      gnn:
        class_path: salt.models.Transformer
        init_args:
          attention:
            class_path: salt.models.GATv2Attention
            init_args:
              num_heads: 8
              head_dim: 16
              activation: *activation
          embd_dim: *embd_dim
          out_proj: False
          num_heads: 8
          num_layers: 8
          residual: true
          norm_layer: *norm_layer
          activation: *activation
          dropout: &dropout 0.1

      pool_net:
        class_path: salt.models.GlobalAttentionPooling
        init_args:
          input_size: *embd_dim

      jet_net:
        class_path: salt.models.ClassificationTask
        init_args:
          name: jet_classification
          label: jets/labels
          loss: torch.nn.CrossEntropyLoss
          net:
            class_path: salt.models.Dense
            init_args:
              input_size: *embd_dim
              output_size: 3
              hidden_layers: [128, 64, 32]
              activation: *activation
              norm_layer: *norm_layer
              dropout: *dropout

      track_net:
        class_path: salt.models.ClassificationTask
        init_args:
          name: track_classification
          label: tracks_loose/labels/truthOriginLabel
          loss:
            class_path: torch.nn.CrossEntropyLoss
            init_args:
              weight: [7.62, 104.91, 1.56, 20.65, 14.05, 14.06, 1.0, 36.07]
          weight: 0.5
          net:
            class_path: salt.models.Dense
            init_args:
              input_size: *embd_dim
              output_size: 8
              hidden_layers: [256, 128, 64]
              activation: *activation
              norm_layer: *norm_layer
              dropout: *dropout
